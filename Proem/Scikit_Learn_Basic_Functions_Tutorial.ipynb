{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 快速开始 Scikit-Learn｜了解 Scikit-Learn 的基础功能并建立一个机器学习模型\n",
    "\n",
    "> 作者: Haohui Que [quehaohui@dp.tech](mailto:quehaohui@dp.tech)\n",
    ">\n",
    "> 创建日期: 2023-03-14 19:13\n",
    ">\n",
    "> 最后一次修改: Haohui Que [quehaohui@dp.tech](mailto:quehaohui@dp.tech), \n",
    ">\n",
    "> 最后一次修改时间: 2023-03-15 12:06\n",
    ">\n",
    "> 目录: /Proem/Scikit_Learn_Basic_Functions_Tutorial.ipynb\n",
    ">\n",
    "> 描述: 本教程可在 Bohrium Notebook 上直接运行。你可以点击界面上方蓝色按钮 `开始连接`，选择 `bohrium-notebook:2023-02-28` 镜像及任何一款节点配置，稍等片刻即可运行。\n",
    "> 如您遇到任何问题，请联系 [bohrium@dp.tech](mailto:bohrium@dp.tech) 。\n",
    ">\n",
    "> 共享协议: 本作品采用[知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议](https://creativecommons.org/licenses/by-nc-sa/4.0/)进行许可。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目标\n",
    "\n",
    "> **了解 Scikit-Learn 在建立机器学习模型中的基础功能，并能够使用 Scikit-Learn 的范式快速建立机器学习模型。。**\n",
    "\n",
    "在学习本教程后，你将能够：\n",
    "\n",
    "- 使用 Scikit-Learn 快速创建某一类型的数据或加载现有数据。\n",
    "- 对数据进行预处理，例如归一化于标准化，以及快速划分数据集。\n",
    "- 进行特征选择的一些方法，例如过滤法、包装法及嵌入法。\n",
    "- 使用 Scikit-Learn 的范式快速建立机器学习模型。\n",
    "- 使用 sklearn.metric 来评价模型质量。\n",
    "- 使用交叉验证以及自动查找最佳的评估器参数。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目录\n",
    "\n",
    "![scikit-learn](https://raw.githubusercontent.com/scikit-learn/scikit-learn/main/doc/logos/scikit-learn-logo.png)\n",
    "\n",
    "* [背景](#background)\n",
    "* [实践](#practice)\n",
    "  * [1 认识 Scikit-Learn](#whatisscikitlearn)\n",
    "    * [1.1 什么是 Scikit-Learn](#1-1)\n",
    "    * [1.2 安装 Scikit-Learn【Bohrium 中可直接运行，无需安装】](#1-2)\n",
    "    * [1.3 验证安装并查看 Scikit-Learn 版本](#1-3)\n",
    "  * [2 Scikit-Learn 方法](#sklearnfeatures)\n",
    "    * [2.1 提供数据集](#2-1)\n",
    "      * [2.1.1 自建数据集](#2-1-1)\n",
    "      * [2.1.2 使用 Scikit-Learn 提供的数据集](#2-1-2)\n",
    "    * [2.2 数据预处理](#2-2)\n",
    "      * [2.2.1 归一化](#2-2-1)\n",
    "      * [2.2.2 标准化](#2-2-2)\n",
    "    * [2.3 快速划分数据集](#2-3)\n",
    "    * [2.4 特征选择](#2-4)\n",
    "      * [2.4.1 过滤法](#2-4-1)\n",
    "      * [2.4.2 包装法](#2-4-2)\n",
    "      * [2.4.3 嵌入法](#2-4-3)\n",
    "    * [2.5 建立模型](#2-5)\n",
    "    * [2.6 评价指标](#2-6)\n",
    "      * [2.6.1 平均绝对误差 MAE](#2-6-1)\n",
    "      * [2.6.2 均方误差 MSE](#2-6-2)\n",
    "      * [2.6.3 决定系数 $R^2$](#2-6-3)\n",
    "    * [2.7 交叉验证](#2-7)\n",
    "    * [2.8 参数优化](#2-8)\n",
    "* [总结](#summary)\n",
    "* [进一步阅读](#furtherreading)\n",
    "* [参考资料](#references)\n",
    "\n",
    "**阅读该教程【最多】约需 30 分钟，让我们开始吧！**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 背景 <a id ='background'></a>\n",
    "\n",
    "本指南的目的是说明 scikit-learn 提供的一些主要功能。Scikit-learn是一个开源机器学习库，支持监督和无监督学习。它还为模型拟合、数据预处理、模型选择、模型评估和许多其他实用程序提供了各种工具。\n",
    "\n",
    "**你需要提前掌握以下知识：**\n",
    "- 非常基本的机器学习实践的工作知识（了解特征、标签、训练集、测试集等）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实践 <a id='practice'></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 认识 Scikit-Learn <a id='whatisscikitlearn'></a>\n",
    "\n",
    "在这一部分，你会了解什么是 Scikit-Learn，在 Bohrium Notebook 中 使用 Scikit-Learn，验证安装并查看版本。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 什么是 Scikit-Learn <a id='1-1'></a>\n",
    "\n",
    "**Scikit-learn**（曾叫做**scikits.learn**还叫做**sklearn**）是基于 [SciPy](https://zh.wikipedia.org/wiki/SciPy) 构建的用于[Python](https://zh.wikipedia.org/wiki/Python)[编程语言](https://zh.wikipedia.org/wiki/编程语言)的[自由软件](https://zh.wikipedia.org/wiki/自由软件)[机器学习](https://zh.wikipedia.org/wiki/机器学习)[库](https://zh.wikipedia.org/wiki/函式庫)。并在 3-Clause BSD许可证下分发。它的特征是具有各种[分类](https://zh.wikipedia.org/wiki/统计分类)、[回归](https://zh.wikipedia.org/wiki/回归分析)和[聚类](https://zh.wikipedia.org/wiki/聚类分析)算法，包括[支持向量机](https://zh.wikipedia.org/wiki/支持向量机)、[随机森林](https://zh.wikipedia.org/wiki/随机森林)、[梯度提升](https://zh.wikipedia.org/wiki/梯度提升技术)、[k-平均聚类](https://zh.wikipedia.org/wiki/K-平均算法)和[DBSCAN](https://zh.wikipedia.org/wiki/DBSCAN)，它被设计协同于Python数值和科学库[NumPy](https://zh.wikipedia.org/wiki/NumPy)和[SciPy](https://zh.wikipedia.org/wiki/SciPy)。[1,2]\n",
    "\n",
    "它具有如下显著特点[3]：\n",
    "\n",
    "* 用于预测数据分析的简单高效的工具 \n",
    "* 每个人都可以访问，并且可以在各种环境中重复使用 \n",
    "* 基于 NumPy、SciPy 和 matplotlib 构建 \n",
    "* 开源，商业上可用 - BSD 许可证"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 安装 Scikit-Learn <a id='1-2'></a>\n",
    "\n",
    "**Bohrium 已默认安装 Scikit-Learn，可在 Notebook 上直接运行。** \n",
    "\n",
    "你可以点击界面上方蓝色按钮 `开始连接`，选择 `notebook-Scikit-Learn:2023-02-28` 镜像及任何一款计算机型，稍等片刻即可运行。\n",
    "\n",
    "如果你的 Bohrium 镜像尚未安装 Scikit-Learn， 最方便的方法是通过 pip 安装:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install scikit-learn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你需要使用更特定于你的平台或包管理器的安装方法，你可以在[这里](https://scikit-learn.org/stable/install.html)查看更完整的安装说明。\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 验证 Scikit-Learn 安装并查看版本 <a id='1-3'></a>\n",
    "\n",
    "安装 Scikit-Learn 后，确认库已成功安装并且你可以开始使用它。 \n",
    "\n",
    "不要跳过此步骤。 \n",
    "\n",
    "如果 Scikit-Learn 未正确安装或在此步骤中引发错误，则将无法运行之后的示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)  # sklearn.__version__ 返回安装的 Scikit-Learn 的版本号"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Scikit-Learn 方法 <a id='sklearnfeatures'></a>\n",
    "\n",
    "在这一节中，你会了解到一些 Scikit-Learn 的基础方法，包括：\n",
    "* 提供数据集\n",
    "* 数据预处理\n",
    "* 快速划分数据集\n",
    "* 特征选择\n",
    "* 建立机器学习模型\n",
    "* 评价指标\n",
    "* 模型选择之交叉验证\n",
    "* 参数优化"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1 提供数据集 <a id='2-1'></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 1.1 自建数据集 <a id='2-1-1'></a>\n",
    "\n",
    "Scikit-Learn 提供了各类分布类型的数据集的建立方法，例如团簇形、环形和月亮形等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 导入基础工具包\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 消除警告\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# sklearn 数据库接口\n",
    "from sklearn.datasets import make_blobs, make_circles, make_classification, make_moons\n",
    "\n",
    "n_samples = 100  # 要创建的样本点\n",
    "data_name = ['blobs', 'circles', 'moons', 'classification']  # 要创建的数据点分布类型\n",
    "\n",
    "datasets = [\n",
    "    make_blobs(n_samples=n_samples, centers=3, random_state=0),\n",
    "    make_circles(n_samples=n_samples, noise=0.2, random_state=0),\n",
    "    make_moons(n_samples=n_samples, noise=0.2, random_state=0),\n",
    "    make_classification(n_samples=n_samples, n_features=2, n_informative=2, n_redundant=0, random_state=0),\n",
    "]\n",
    "\n",
    "# 查看我们构建的数据集\n",
    "fig, ax = plt.subplots(ncols=4, nrows=1, figsize=(12,3))  # 实例化一个 4 列 1 行的图片，尺寸为 长*宽=12*3\n",
    "for i, (X,Y) in enumerate(datasets):\n",
    "    ax[i].scatter(X[:,0],X[:,1], c=Y, s=10, cmap=\"rainbow\")\n",
    "    ax[i].set_title(data_name[i])\n",
    "    ax[i].set_xticks (())\n",
    "    ax[i].set_yticks (())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 1.2 使用 sklearn 提供的数据集（以鸢尾花数据集为例） <a id='2-1-2'></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "sklearn.datasets 模块提供了一系列加载和获取著名数据集如鸢尾花、波士顿房价、Olivetti人脸、MNIST数据集等的工具。\n",
    "\n",
    "鸢尾花识别是一个经典的机器学习分类问题，它的数据样本中包括了4个特征变量，1个类别变量，样本总数为150。\n",
    "\n",
    "它的目标是为了根据花萼长度（sepal length）、花萼宽度（sepal width）、花瓣长度（petal length）、花瓣宽度（petal width）这四个特征来识别出鸢尾花属于山鸢尾（iris-setosa）、变色鸢尾（iris-versicolor）和维吉尼亚鸢尾（iris-virginica）中的哪一种。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X_iris = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "X_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Y_iris = pd.DataFrame(iris.target, columns=['Kind'])\n",
    "Y_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris  # 查看更多信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 将特征与标签在同一个 Pandas DataFrame 中表示。\n",
    "df_iris = pd.concat([X_iris, Y_iris], axis=1)  # 合并特征表与标签表\n",
    "df_iris"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2 数据预处理（以标准化和归一化举例） <a id='2-2'></a>\n",
    "\n",
    "在机器学习算法实践中，我们往往有着将不同规格的数据转换到同一规格，或不同分布的数据转换到某个特定分布的需求，这种需求统称为将数据“无量纲化”。\n",
    "\n",
    "* 譬如梯度和矩阵为核心的算法中，譬如逻辑回归，支持向量机，神经网络，无量纲化可以加快求解速度；\n",
    "* 而在距离类模型，譬如K近邻，K-Means聚类中，无量纲化可以帮我们提升模型精度，避免某一个取值范围特别大的特征对距离计算造成影响。\n",
    "* （一个特例是决策树和树的集成算法们，对决策树我们不需要无量纲化，决策树可以把任意数据都处理得很好。）\n",
    "\n",
    "数据的无量纲化可以是线性的，也可以是非线性的。\n",
    "\n",
    "线性的无量纲化包括**中心化 (Zero-centered或者Mean-subtraction) 处理**和**缩放处理 (Scale)**。\n",
    "\n",
    "中心化的本质是让所有记录减去一个固定值，即让数据样本平移到某个位置。\n",
    "\n",
    "缩放的本质是通过除以一个固定值，将数据固定在某个范围之中，取对数也算是一种缩放处理。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 2.1 归一化 <a id='2-2-1'></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "当数据(x)按照最小值中心化后，再按极差（最大值 - 最小值）缩放，数据移动了最小值个单位，并且会被收敛到[0,1]之间。\n",
    "\n",
    "这个过程，就叫做数据归一化(Normalization，又称 Min-Max Scaling)。\n",
    "\n",
    "注意，Normalization 是归一化，不是正则化，真正的正则化是 regularization，不是数据预处理的一种手段。\n",
    "\n",
    "归一化之后的数据服从正态分布，公式如下：\n",
    "\n",
    "$$\n",
    "x^* = \\frac{x-min(x)}{max(x)-min(x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "手动来实现一个归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[-1, 2], [-0.5, 6], [0, 10], [1, 18]])\n",
    "\n",
    "# 归一化\n",
    "X_nor = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "X_nor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#逆转归一化\n",
    "X_returned = X_nor * (X.max(axis=0) - X.min(axis=0)) + X.min(axis=0)\n",
    "X_returned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "使用 sklearn 来实现归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
    "\n",
    "# 能够判断data的结构吗？\n",
    "# 如果换成表是什么样子？\n",
    "import pandas as pd\n",
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#实现归一化\n",
    "scaler = MinMaxScaler(feature_range=(0,1))         # 实例化, feature_range 确定 MinMaxScaler 缩放的范围\n",
    "scaler = scaler.fit(data)                           # fit，在这里本质是生成 min(x) 和 max(x)，当X中的特征数量非常多的时候，fit 会报错并表示，此时使用 partial_fit 作为训练接口 scaler = scaler.partial_fit(data)\n",
    "result = scaler.transform(data)                     # 通过接口导出结果\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_ = scaler.fit_transform(data)                # 训练和导出结果一步达成\n",
    "result_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler.inverse_transform(result)                    # 将归一化后的结果逆转"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 2.2 标准化 <a id='2-2-2'></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "当数据 (x) 按均值 (μ) 中心化后，再按标准差 (σ) 缩放，数据就会服从为均值为 0，方差为 1 的正态分布（即标准正态分布）\n",
    "\n",
    "这个过程，就叫做数据标准化 (Standardization，又称 Z-score normalization)，\n",
    "\n",
    "公式如下：\n",
    "$$\n",
    "x^*=\\frac{x-\\mu}{\\sigma}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = [[-1, 2], [-0.5, 6], [0, 10], [1, 18]]\n",
    "\n",
    "scaler = StandardScaler()                           # 实例化\n",
    "scaler.fit(data)                                    # fit的本质是生成均值和方差\n",
    "print(scaler.mean_)                                 # 查看均值的属性 mean_\n",
    "print(scaler.var_)                                  # 查看方差的属性 var_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_std = scaler.transform(data)                      # 通过接口导出结果\n",
    "print(x_std.mean())                                 # 导出的结果是一个数组，用 mean() 查看均值\n",
    "print(x_std.std())                                  # 用 std() 查看方差\n",
    "x_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler.fit_transform(data)                          # 使用 fit_transform(data)一步达成结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scaler.inverse_transform(x_std)                     # 使用 inverse_transform 逆转标准化"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3 快速划分数据集 <a id='2-3'></a>\n",
    "\n",
    "可以使用 `sklearn.model_selection.train_test_split` 方法快速划分训练集和测试集。\n",
    "\n",
    "在本例中，我们使用生物炭吸附数据集作为演示。\n",
    "\n",
    "该数据集根据生物炭的特性、金属来源和环境条件等预测生物炭系统的重金属吸附效率。\n",
    "\n",
    "数据将通过 Pandas 自动下载，如果你想了解更多内容，请参阅：\n",
    "\n",
    "- [生物炭数据集简介](https://dptechnology.feishu.cn/docx/FZvNdfARFouEBqxsGR8cTA7wnDf)\n",
    "- [来源论文](https://doi.org/10.1016/j.chemosphere.2021.130204)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "# 导入数据集\n",
    "df_bio = pd.read_csv('https://dp-public.oss-cn-beijing.aliyuncs.com/community/AI4EC/biochar_sorption(Original).csv')\n",
    "df_bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 划分特征与标签\n",
    "X_mat = df_bio.iloc[:, :13].values\n",
    "Y_mat = df_bio.iloc[:, -1].values.reshape(-1, 1)\n",
    "# 对数据集标准化\n",
    "X = StandardScaler().fit_transform(X_mat)\n",
    "Y = StandardScaler().fit_transform(Y_mat)\n",
    "# 划分训练集和测试集\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.3, random_state=0)  # random_state 参数规定每次以相同的方式进行数据划分"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 4 特征选择 <a id='2-4'></a>\n",
    "\n",
    "当数据预处理完成后，我们需要选择有意义的特征输入机械学习的算法和模型中进行训练。通常来说，从两个方面考虑来选择特征:\n",
    "\n",
    "1. 特征是否发散：如果一个特征不发散，例如方差接近于0或者基本上都是重复数据，也就是说样本在这个特征上基本上没有差异，这个特征对于样本区分并没有什么用。\n",
    "2. 特征与目标的相关性：这点比较显而易见，与目标相关性高的特征，应当优选选择。\n",
    "\n",
    "最近邻算法 KNN，单棵决策树，支持向量机SVM，神经网络，回归算法，都需要遍历特征或升维来进行运算，所以他们本身的运算量就很大，需要的时间就很长，因此方差过滤这样的特征选择对他们来说就尤为重要。\n",
    "\n",
    "但对于不需要遍历特征的算法，比如随机森林，它随机选取特征进行分枝，本身运算就非常快速，因此特征选择对它来说效果平平。\n",
    "\n",
    "**因此，过滤法的主要对象是：需要遍历特征或升维的算法们，而过滤法的主要目的是：在维持算法表现的前提下，帮助算法降低计算成本。**\n",
    "\n",
    "根据特征选择的形式又可以将特征选择的方法分为3种："
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 4.1 Filter : 过滤法，按照发散性或者相关性对各个特征进行评分，设定阀值或者待选阀值的个数，选择特征。 <a id='2-4-1'></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### 4.1.1 方差法(计算各个特征的方差，根据阀值，选择方差大于阀值的特征)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold  # 方差选择法，返回值为特征选择后的数据\n",
    "\n",
    "X_var = VarianceThreshold(threshold = 2).fit_transform(X_mat)  # 参数threshold为方差的阀值\n",
    "X_var.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### 4.1.2 卡方过滤 (分类算法相关性检验)\n",
    "\n",
    "卡方过滤是专门针对离散型标签（即分类问题）的相关性过滤。\n",
    "\n",
    "卡方检验类 featureselection.chi2 计算每个非负特征和标签之间的卡方统计量，并依照卡方统计量由高到低为特征排名。\n",
    "\n",
    "再结合 featureselection.SelectkBest 这个可以输入“评分标准\"来选出前 K 个分数最高的特征的类，我们可以借此除去最可能独立于标签，与我们分类目的无关的特征。\n",
    "\n",
    "卡方检验要求特征中均为非负值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest  # 选择K个最好的特征，返回选择特征后的数据\n",
    "from sklearn.feature_selection import chi2  # 卡方检验\n",
    "\n",
    "X_fschi = SelectKBest(chi2, k = 2).fit_transform(X_iris, Y_iris)\n",
    "# 第一个参数为计算评估特征依赖的统计量\n",
    "# 第二个参数指定选择前 k 个特征\n",
    "\n",
    "X_fschi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "从特征工程的角度，我们希望选取卡方值很大，p 值小于 0.05 的特征，即和标签是相关联的特征。\n",
    "\n",
    "而调用 SelectkBest 之前，我们可以直接从 chi2 实例化后的模型中获得各个特征所对应的卡方值和 P 值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chivalue, pvalues_chi = chi2(X_iris, Y_iris)  # 使用卡方系数查看相关性， chivalue 是卡方值，pvalues_chi 是 P值\n",
    "pvalues_chi\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### 4.1.3 F 检验 (线性相关性)\n",
    "\n",
    "F检验，又称ANOVA，方差齐性检验，是用来捕捉每个特征与标签之间的 **线性关系** 的过滤方法。\n",
    "\n",
    "F检验的本质是寻找两组数据之间的线性关系，其原假设是'数据不存在显著的线性关系”。\n",
    "\n",
    "它返回 F 值和 P 值两个统计量。\n",
    "\n",
    "和卡方过滤一样，我们希望选取 p 值小于 0.05 或 0.01 的特征，这些特征与标签是显著线性相关的，\n",
    "\n",
    "而 P 值大于 0.05 或 0.01 的特征则被我们认为是和标签没有显著线性关系的特征，应该被删除。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest  # 选择K个最好的特征，返回选择特征后的数据\n",
    "from sklearn.feature_selection import f_classif  # F检验\n",
    "\n",
    "F, pvalues_F = f_classif(X_iris, Y_iris)\n",
    "print(pvalues_F)\n",
    "\n",
    "x_F = SelectKBest(chi2, k = 2).fit_transform(X_iris, Y_iris)\n",
    "x_F\n",
    "\n",
    "# cross_val_score(RFR(n_estimators=10, random_state=0), X_F, Y, cv=5).mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### 4.1.4 互信息法\n",
    "\n",
    "互信息法是用来捕捉每个特征与标签之间的任意关系（包括线性和非线性关系）的过滤方法。\n",
    "\n",
    "和F检验相似，它既可以做回归也可以做分类，并且包含两个类featureselection.mutualinfoclassif（互信息分类）和 featureselection.mutualinforegression（互信息回归）。\n",
    "\n",
    "这两个类的用法和参数都和F检验一模一样，不过互信息法比F检验更加强大，F检验只能够找出线性关系，而互信息法可以找出任意关系。 \n",
    "\n",
    "互信息法不返回 p 值或 F 值类似的统计量，它返口\"每个特征与目标之间的互信息量的估计”，\n",
    "\n",
    "这个估计量在 [0,1] 之间取值，为 0 则表示两个变量独立，为 1 则表示两个变量完全相关。以互信息分类为例的代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif as MIC\n",
    "\n",
    "MIC(X_iris, Y_iris)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 4.2 Wrapper : 包装法，根据目标函数(通常是预测效果评分)，每次选择若干特征，或者排除若干特征。 <a id='2-4-2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### 4.2.1 递归特征消除法\n",
    "\n",
    "递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "RFE(estimator = LogisticRegression(), n_features_to_select = 2).fit_transform(iris.data, iris.target)\n",
    "# 递归特征消除法，返回特征选择后的数据\n",
    "# 参数 estimator 为基模型\n",
    "# 参数n_features_to_select为选择的特征个数"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 4.3 Embedded 嵌入法 <a id='2-4-3'></a>\n",
    "\n",
    "嵌入法是一种让算法自己決定使用哪些特征的方法，即特征选择和算法训练同时进行。\n",
    "\n",
    "在使用嵌入法时，我们先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数.\n",
    "\n",
    "根据权值系数从大到小选择权值系数大的特征，权值系数往往代表了特征对于模型的某种贡献或重要性。\n",
    "\n",
    "selectFromModel 是一个元变换器，可以与任何在拟合后具有 coef_，feature_importances_ 属性或具有惩罚项的评估器一起使用\n",
    "\n",
    "（比如随机森林和树模型就具有属性 featureimportances_，逻辑回归就带有l1和l2惩罚项，线性支持向量机也支持l2惩罚项）。\n",
    "\n",
    "| **参数**     | **说明**                                                     |\n",
    "| ------------ | ------------------------------------------------------------ |\n",
    "| estimator    | 使用的模型评估器，  只要是带  coef_，feature_importances_ 属性或具有惩罚项的评估器都可以使用。 |\n",
    "| threshold    | 特征重要性的阈值，重要性低于这个阈值的特征都将被删除。       |\n",
    "| prefit       | 默认False，判断是否将实例化后的模型直接传递给构造函数。如果为True，则必须直接调用  fit  和  transform，不能使用  fit_transform，并且  SelectFromModel  不能与  cross_val_score,  GridSearchCV和克隆估计器的类似程序一起使用。 |\n",
    "| norm_order   | k可输入非零整数，正无穷，负无穷，默认值为1在评估器的  coef_  属性高于一维的情况下，用于过滤低于阈值的系数的向量的范数的阶数。 |\n",
    "| max_features | 在阈值设定下，要选择的最大特征数。要禁用阈值并仅根据  max_features  选择，设 置threshold  = -np.inf |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfr = RandomForestRegressor(n_estimators=10, random_state=0)\n",
    "print(rfr.fit(X,Y).feature_importances_)  # 打印各特征的重要性参数\n",
    "X_embedded = SelectFromModel(estimator=rfr, threshold=0.005).fit_transform(X, Y)\n",
    "X_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ## ######### TIME WARNING: 30 seconds ###########\n",
    "threshold = np.linspace(0, rfr.fit(X, Y).feature_importances_.max(), 50)\n",
    "scores = []\n",
    "\n",
    "for i in threshold:\n",
    "    X_embedded = SelectFromModel(rfr, threshold=i).fit_transform(X, Y)\n",
    "    cv_score = cross_val_score(rfr, X_embedded,Y, cv=5).mean()\n",
    "    scores.append(cv_score)\n",
    "\n",
    "plt.plot(threshold, scores)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 5 建立机器学习模型 <a id='2-5'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression  # 导入线性回归模块\n",
    "\n",
    "lr = LinearRegression()  # 实例化一个模型, 参数都为默认值\n",
    "lr.fit(Xtrain, Ytrain)  # 用训练集数据进行训练\n",
    "Ypred = lr.predict(Xtest)  # 对测试结果进行预测\n",
    "lr.score(Xtest, Ytest)  # 查看预测分数\n",
    "print(f'The predict score is {lr.score(Xtest, Ytest)}')\n",
    "\n",
    "# 绘制预测结果图\n",
    "plt.plot([-0.8,3.5], [-0.8,3.5], \"--k\")\n",
    "plt.scatter(x=Ytest, y=Ypred, c='orange')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor  # 导入需要的模块\n",
    "\n",
    "rfr = RandomForestRegressor()  # 实例化一个模型, 参数都为默认值\n",
    "rfr.fit(Xtrain, Ytrain)  # 用训练集数据进行训练\n",
    "Ypred = rfr.predict(Xtest)  # 对测试结果进行预测\n",
    "rfr.score(Xtest, Ytest)  # 查看预测分数\n",
    "print(f'The predict score is {rfr.score(Xtest, Ytest)}')\n",
    "\n",
    "# 绘制预测结果图\n",
    "plt.plot([-0.8,3.5], [-0.8,3.5], \"--k\")\n",
    "plt.scatter(x=Ytest, y=Ypred, c='orange')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "可以看到，选择不同的机器学习模型，对预测效果存在重要影响。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 6 评价指标 <a id='2-6'></a>\n",
    "\n",
    "模型效果如何需要有效的评估方法。在分类算法中，我们可以很明确的使用准确率指标，而在回归类算法中，我们需要一些指标来评估是否预测到了正确的数值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! pip install scikit-learn==1.2.0\n",
    "from sklearn.metrics import get_scorer_names\n",
    "print(get_scorer_names())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 6.1 平均绝对误差 MAE <a id='2-6-1'></a>\n",
    "$$\n",
    "\\operatorname{MAE}(y, \\hat{y})=\\frac{1}{n_{\\text {samples }}} \\sum_{i=0}^{n_{\\text {samples }}-1}\\left|y_i-\\hat{y}_i\\right|\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error as MAE\n",
    "\n",
    "MAE(Ypred, Ytest)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 6.2 均方误差 MSE <a id='2-6-2'></a>\n",
    "\n",
    "$$\n",
    "M S E=\\frac{\\sum_{i=0}^{N-1}\\left[\\hat{y}-y_i\\right]^2}{N}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "MSE(Ypred, Ytest)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 6.3 决定系数 $R^2$ <a id='2-6-3'></a>\n",
    "\n",
    "$$\n",
    "R^2(y, \\hat{y})=1-\\frac{\\sum_{i=0}^{n_{\\text {samples-1}}}\\left(y_i-\\hat{y}_i\\right)^2}{\\sum_{i=0}^{n_{\\text {samples}-1}}\\left(y_i-\\bar{y}\\right)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score as R2\n",
    "\n",
    "R2(Ypred, Ytest)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 7 交叉验证 <a id='2-7'></a>\n",
    "\n",
    "学习预测函数的参数并在相同的数据上对其进行测试是一个方法上的错误：一个只重复它刚刚看到的样本标签的模型将获得满分，但无法预测任何有用的数据。\n",
    "\n",
    "这种情况称为过拟合。\n",
    "\n",
    "为了避免这种情况，在执行（监督）机器学习 “实验” 时，通常的做法是将部分可用数据保留为测试集 X_test，y_test。\n",
    "\n",
    "下面是模型训练中典型交叉验证工作流的示意图。最佳参数可以通过网格搜索确定。\n",
    "\n",
    "![CV-sklearn](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)\n",
    "\n",
    "由 k 折交叉验证报告的评估指标是循环后得到的平均值。\n",
    "\n",
    "这种方法的计算成本可能很高，但不会浪费太多数据，这在诸如样本数量非常少的问题中仍能较好的反应模型效果，是交叉验证的一个主要优势。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "\n",
    "cross_val_score(rfr, X, Y, cv=5, scoring='neg_mean_squared_error')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 8 参数优化 <a id='2-8'></a>\n",
    "\n",
    "所有估计器都有可以调整的参数（在文献中通常称为超参数）。\n",
    "\n",
    "估计器的泛化功效通常主要取决于几个参数。\n",
    "\n",
    "例如，RandomForestRegressor 具有一个确定林中树数的 n_estimators 参数，以及一个确定每棵树最大深度的 max_depth 参数。\n",
    "\n",
    "通常，不清楚这些参数的确切值应该是多少，因为它们取决于手头的数据。\n",
    "\n",
    "Scikit-Learn提供了自动查找最佳参数组合的工具。在下面的示例中，我们使用 [GridSearchCV](https://scikit-learn.org/stable/modules/grid_search.html#exhaustive-grid-search) 方法随机搜索支持向量机分类的参数空间。\n",
    "\n",
    "搜索结束后，GridSearchCV 就像一个 [SVC](https://scikit-learn.org/stable/modules/svm.html#classification)，它已经拟合了最佳参数集。\n",
    "\n",
    "你可在[用户指南](https://scikit-learn.org/stable/modules/grid_search.html#grid-search)中了解更多内容："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# ## ################ Time Warning: 20 seconds ###################\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "threshold = 0.2000  # 设置一个分类阈值\n",
    "df_bio.loc[df_bio['SEoHM'] >= threshold, 'Effect'] = 1  # 根据吸附效率对材料进行二分类\n",
    "df_bio.loc[df_bio['SEoHM'] < threshold, 'Effect'] = 0  # 根据吸附效率对材料进行二分类\n",
    "\n",
    "X = StandardScaler().fit_transform(df_bio.iloc[:, :13])  # 对特征进行标准化\n",
    "\n",
    "# 设置网格搜索的超参数空间\n",
    "gamma_range = np.logspace(-2,2,20)\n",
    "coef0_range = np.linspace(0,5,20)\n",
    "param_grid = dict(gamma = gamma_range, coef0 = coef0_range)\n",
    "\n",
    "# 进行交叉验证\n",
    "cv = StratifiedShuffleSplit(n_splits=10, test_size=0.3, random_state=420)\n",
    "# 使用网格搜索寻找最优参数并使用其训练模型\n",
    "grid = GridSearchCV(SVC(kernel=\"poly\", degree=1), param_grid=param_grid, cv=cv)\n",
    "grid.fit(X, df_bio['Effect'])\n",
    "\n",
    "# 得到最优参数及最优模型预测分数\n",
    "print(f\"最优的参数是： {grid.best_params_} ，其得分为 {grid.best_score_}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 总结 <a id='summary'></a>\n",
    "\n",
    "在本教程中，您学习了在 Scikit-Learn 的一些基础方法。 \n",
    "\n",
    "具体而言，您了解到： \n",
    "- 使用 Scikit-Learn 快速创建某一类型的数据或加载现有数据。\n",
    "- 对数据进行预处理，例如归一化于标准化，以及快速划分数据集。\n",
    "- 进行特征选择的一些方法，例如过滤法、包装法及嵌入法。\n",
    "- 使用 Scikit-Learn 的范式快速建立机器学习模型。\n",
    "- 使用 sklearn.metric 来评价模型质量。\n",
    "- 使用交叉验证以及自动查找最佳的评估器参数。\n",
    " \n",
    "你有什么问题吗？ 欢迎与我们联系 [bohrium@dp.tech](mailto:bohrium@dp.tech) 。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 进一步阅读 <a id='furtherreading'></a>\n",
    "\n",
    "如果您希望更深入学习 Scikit-Learn 及机器学习，本节提供有关该主题的更多资源。\n",
    "\n",
    "**书籍**\n",
    "\n",
    "- [Machine Learning](https://link.springer.com/book/10.1007%2F978-981-15-1967-3), 2021.\n",
    "\n",
    "**Scikit-Learn 项目**\n",
    "\n",
    "- [Scikit-Learn Homepage](https://scikit-learn.org/stable/).\n",
    "- [Scikit-Learn Documentation](https://scikit-learn.org/stable/user_guide.html)\n",
    "- [Scikit-Learn Installation Guide](https://scikit-learn.org/stable/install.html)\n",
    "- [Scikit-Learn, Wikipedia](https://zh.wikipedia.org/wiki/Scikit-learn).\n",
    "- [Scikit-Learn on GitHub](https://github.com/scikit-learn/scikit-learn)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考\n",
    "\n",
    "1. https://github.com/scikit-learn/scikit-learn\n",
    "2. https://zh.wikipedia.org/wiki/Scikit-learn\n",
    "3. https://scikit-learn.org/stable/index.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
