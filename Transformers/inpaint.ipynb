{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Text-guided image-inpainting","metadata":{}},{"cell_type":"markdown","source":"> 最后一次修改: [dingzh@dp.tech](mailto:dingzh@dp.tech)\n>\n> 描述: 本教程主要参考 [hugging face notebook](https://github.com/huggingface/notebooks/blob/main/diffusers_doc/en/inpaint.ipynb)，可在 Bohrium Notebook 上直接运行。你可以点击界面上方蓝色按钮 `开始连接`，选择 `bohrium-notebook:2023-04-07` 镜像及任意一款`GPU`节点配置，稍等片刻即可运行。\n> 如您遇到任何问题，请联系 [bohrium@dp.tech](mailto:bohrium@dp.tech) 。\n>\n> 共享协议: 本作品采用[知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议](https://creativecommons.org/licenses/by-nc-sa/4.0/)进行许可。\n\nThis notebook shows how to fine-tune any pretrained Vision model for Image Classification on a custom dataset. The idea is to add a randomly initialized classification head on top of a pre-trained encoder, and fine-tune the model altogether on a labeled dataset.","metadata":{}},{"cell_type":"markdown","source":"The [StableDiffusionInpaintPipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/inpaint#diffusers.StableDiffusionInpaintPipeline) allows you to edit specific parts of an image by providing a mask and a text prompt. It uses a version of Stable Diffusion, like [`runwayml/stable-diffusion-inpainting`](https://huggingface.co/runwayml/stable-diffusion-inpainting) specifically trained for inpainting tasks.\n\nGet started by loading an instance of the [StableDiffusionInpaintPipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/inpaint#diffusers.StableDiffusionInpaintPipeline):","metadata":{}},{"cell_type":"code","source":"pip install diffusers transformers","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os \nos.environ['HTTP_PROXY'] = 'http://ga.dp.tech:8118'\nos.environ['HTTPS_PROXY'] = 'http://ga.dp.tech:8118'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import PIL\nimport requests\nimport torch\nfrom io import BytesIO\n\nfrom diffusers import StableDiffusionInpaintPipeline\n\npipeline = StableDiffusionInpaintPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-inpainting\",\n    torch_dtype=torch.float16,\n)\npipeline = pipeline.to(\"cuda\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Download an image and a mask of a dog which you'll eventually replace:","metadata":{}},{"cell_type":"code","source":"def download_image(url):\n    response = requests.get(url)\n    return PIL.Image.open(BytesIO(response.content)).convert(\"RGB\")\n\n\nimg_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\"\nmask_url = \"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\"\n\ninit_image = download_image(img_url).resize((512, 512))\nmask_image = download_image(mask_url).resize((512, 512))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now you can create a prompt to replace the mask with something else:","metadata":{}},{"cell_type":"code","source":"prompt = \"Face of a yellow cat, high resolution, sitting on a park bench\"\nimage = pipeline(prompt=prompt, image=init_image, mask_image=mask_image).images[0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`image`          | `mask_image` | `prompt` | output |\n:-------------------------:|:-------------------------:|:-------------------------:|-------------------------:|\n<img src=\"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png\" alt=\"drawing\" width=\"250\"/> | <img src=\"https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png\" alt=\"drawing\" width=\"250\"/> | ***Face of a yellow cat, high resolution, sitting on a park bench*** | <img src=\"https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/in_paint/yellow_cat_sitting_on_a_park_bench.png\" alt=\"drawing\" width=\"250\"/> |\n\n\n<Tip warning={true}>\n\nA previous experimental implementation of inpainting used a different, lower-quality process. To ensure backwards compatibility, loading a pretrained pipeline that doesn't contain the new model will still apply the old inpainting method.\n\n</Tip>\n\nCheck out the Spaces below to try out image inpainting yourself!\n\n<iframe\n\tsrc=\"https://runwayml-stable-diffusion-inpainting.hf.space\"\n\tframeborder=\"0\"\n\twidth=\"850\"\n\theight=\"500\"\n></iframe>","metadata":{}}]}