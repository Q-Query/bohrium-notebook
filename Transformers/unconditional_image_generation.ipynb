{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Unconditional image generation","metadata":{}},{"cell_type":"markdown","source":"Unconditional image generation is a relatively straightforward task. The model only generates images - without any additional context like text or an image - resembling the training data it was trained on.\n\nThe [DiffusionPipeline](https://huggingface.co/docs/diffusers/main/en/api/diffusion_pipeline#diffusers.DiffusionPipeline) is the easiest way to use a pre-trained diffusion system for inference.\n\nStart by creating an instance of [DiffusionPipeline](https://huggingface.co/docs/diffusers/main/en/api/diffusion_pipeline#diffusers.DiffusionPipeline) and specify which pipeline checkpoint you would like to download.\nYou can use any of the ðŸ§¨ Diffusers [checkpoints](https://huggingface.co/models?library=diffusers&sort=downloads) from the Hub (the checkpoint you'll use generates images of butterflies).\n\n<Tip>\n\nðŸ’¡ Want to train your own unconditional image generation model? Take a look at the training [guide](https://huggingface.co/docs/diffusers/main/en/using-diffusers/training/unconditional_training) to learn how to generate your own images.\n\n</Tip>\n\nIn this guide, you'll use [DiffusionPipeline](https://huggingface.co/docs/diffusers/main/en/api/diffusion_pipeline#diffusers.DiffusionPipeline) for unconditional image generation with [DDPM](https://arxiv.org/abs/2006.11239):","metadata":{}},{"cell_type":"code","source":"pip install diffusers transformers","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os \nos.environ['HTTP_PROXY'] = 'http://ga.dp.tech:8118'\nos.environ['HTTPS_PROXY'] = 'http://ga.dp.tech:8118'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from diffusers import DiffusionPipeline\n\ngenerator = DiffusionPipeline.from_pretrained(\"anton-l/ddpm-butterflies-128\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The [DiffusionPipeline](https://huggingface.co/docs/diffusers/main/en/api/diffusion_pipeline#diffusers.DiffusionPipeline) downloads and caches all modeling, tokenization, and scheduling components. \nBecause the model consists of roughly 1.4 billion parameters, we strongly recommend running it on a GPU.\nYou can move the generator object to a GPU, just like you would in PyTorch:","metadata":{}},{"cell_type":"code","source":"generator.to(\"cuda\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now you can use the `generator` to generate an image:","metadata":{}},{"cell_type":"code","source":"image = generator().images[0]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The output is by default wrapped into a [`PIL.Image`](https://pillow.readthedocs.io/en/stable/reference/Image.html?highlight=image#the-image-class) object.\n\nYou can save the image by calling:","metadata":{}},{"cell_type":"code","source":"image.save(\"generated_image.png\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Try out the Spaces below, and feel free to play around with the inference steps parameter to see how it affects the image quality!\n\n<iframe\n\tsrc=\"https://stevhliu-ddpm-butterflies-128.hf.space\"\n\tframeborder=\"0\"\n\twidth=\"850\"\n\theight=\"500\"\n></iframe>","metadata":{}}]}