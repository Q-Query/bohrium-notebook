{"metadata":{"colab":{"collapsed_sections":[],"name":"OWL-ViT-inference example.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Getting started with Owl-ViT\n> 作者: Zhaohan Ding [dingzh@dp.tech](mailto:dingzh@dp.tech)\n>\n> 创建日期: 2023-04-18 18:35\n>\n> 最后一次修改: Zhaohan Ding [dingzh@dp.tech](mailto:dingzh@dp.tech), \n>\n> 最后一次修改时间: 2023-04-18 18:35\n>\n> 描述: 本教程主要参考 [1]，可在 Bohrium Notebook 上直接运行。你可以点击界面上方蓝色按钮 `开始连接`，选择 `bohrium-notebook:2023-04-07` 镜像及任意一款`GPU`节点配置，稍等片刻即可运行。\n> 如您遇到任何问题，请联系 [bohrium@dp.tech](mailto:bohrium@dp.tech) 。\n>\n> 共享协议: 本作品采用[知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议](https://creativecommons.org/licenses/by-nc-sa/4.0/)进行许可。\nIn this notebook, we are going to run the [OWL-ViT](https://arxiv.org/abs/2205.06230) model (an open-vocabulary object detection model) by Google Research on scikit-image samples images. \n\n## OWL-ViT: A Quick Intro\nOWL-ViT is an open-vocabulary object detector. Given an image and one or multiple free-text queries, it finds objects matching the queries in the image. Unlike traditional object detection models, OWL-ViT is not trained on labeled object datasets and leverages multi-modal representations to perform open-vocabulary detection. \n\nOWL-ViT uses CLIP with a ViT-like Transformer as its backbone to get multi-modal visual and text features. To use CLIP for object detection, OWL-ViT removes the final token pooling layer of the vision model and attaches a lightweight classification and box head to each transformer output token. Open-vocabulary classification is enabled by replacing the fixed classification layer weights with the class-name embeddings obtained from the text model. The authors first train CLIP from scratch and fine-tune it end-to-end with the classification and box heads on standard detection datasets using a bipartite matching loss. One or multiple text queries per image can be used to perform zero-shot text-conditioned object detection.\n\n![owlvit architecture](https://raw.githubusercontent.com/google-research/scenic/a41d24676f64a2158bfcd7cb79b0a87673aa875b/scenic/projects/owl_vit/data/owl_vit_schematic.png)","metadata":{"id":"-Wc92cWK-Aas"}},{"cell_type":"markdown","source":"## Set-up environment\n\nFirst, we install the HuggingFace Transformers library (from source for now, as the model was recently added to the library and is under active development). This might take a few minutes.","metadata":{"id":"uIcaig48T6yv"}},{"cell_type":"code","source":"!pip install -q git+https://github.com/huggingface/transformers.git","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_XLma_DL3S9-","outputId":"3d8e2639-90a0-4820-bf7f-04eee42e5258"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Optional:** Install Pillow, matplotlib and OpenCV if you are running this notebook locally.","metadata":{}},{"cell_type":"code","source":"# !pip install Pillow\n# !pip install matplotlib\n!pip install opencv-python","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We also quickly upload some telemetry - this tells us which examples and software versions are getting used so we know where to prioritize our maintenance efforts. We don't collect (or care about) any personally identifiable information, but if you'd prefer not to be counted, feel free to skip this step or delete this cell entirely.","metadata":{}},{"cell_type":"code","source":"from transformers.utils import send_example_telemetry\n\nsend_example_telemetry(\"zeroshot_object_detection_with_owlvit_notebook\", framework=\"pytorch\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load pre-trained model and processor\n\nLet's first apply the image preprocessing and tokenize the text queries using `OwlViTProcessor`. The processor will resize the image(s), scale it between [0-1] range and normalize it across the channels using the mean and standard deviation specified in the original codebase.\n\n\nText queries are tokenized using a CLIP tokenizer and stacked to output tensors of shape [batch_size * num_max_text_queries, sequence_length]. If you are inputting more than one set of (image, text prompt/s), num_max_text_queries is the maximum number of text queries per image across the batch. Input samples with fewer text queries are padded. ","metadata":{"id":"QPrCVnimE0qR"}},{"cell_type":"code","source":"from transformers import OwlViTProcessor, OwlViTForObjectDetection\n\nmodel = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\")\nprocessor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AD8DXCnJ7faH","outputId":"3207f732-be55-46a2-ce3e-346e32efeac9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocess input image and text queries\n\nLet's use the image of astronaut Eileen Collins to test OWL-ViT. It's part of the [NASA](https://www.nasa.gov/multimedia/imagegallery/index.html) Great Images dataset.\n\nYou can use one or multiple text prompts per image to search for the target object(s). Let's start with a simple example where we search for multiple objects in a single image.","metadata":{"id":"7QN-vURe3euV"}},{"cell_type":"code","source":"import cv2\nimport skimage\nimport numpy as np\nfrom PIL import Image\n\n# Download sample image\nimage = skimage.data.astronaut()\nimage = Image.fromarray(np.uint8(image)).convert(\"RGB\")\n\n# Text queries to search the image for\ntext_queries = [\"human face\", \"rocket\", \"nasa badge\", \"star-spangled banner\"]\n\nimage","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":529},"id":"LOX1__3nrezW","outputId":"4435e2c9-b79d-4aaf-d7ef-b1cf2c239d01"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\n# Use GPU if available\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\nelse:\n    device = torch.device(\"cpu\")","metadata":{"id":"_Fu_0bM2Mz0R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Process image and text inputs\ninputs = processor(text=text_queries, images=image, return_tensors=\"pt\").to(device)\n\n# Print input names and shapes\nfor key, val in inputs.items():\n    print(f\"{key}: {val.shape}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S6lQ2d9uodiv","outputId":"f5634352-d44e-47c4-f66c-4ad51522efdb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Forward pass\n\nNow we can pass the inputs to our OWL-ViT model to get object detection predictions. \n\n`OwlViTForObjectDetection` model outputs the prediction logits, boundary boxes and class embeddings, along with the image and text embeddings outputted by the `OwlViTModel`, which is the CLIP backbone.","metadata":{"id":"ten5ZJQsoUbE"}},{"cell_type":"code","source":"# Set model in evaluation mode\nmodel = model.to(device)\nmodel.eval()\n\n# Get predictions\nwith torch.no_grad():\n  outputs = model(**inputs)\n\nfor k, val in outputs.items():\n    if k not in {\"text_model_output\", \"vision_model_output\"}:\n        print(f\"{k}: shape of {val.shape}\")\n\nprint(\"\\nText model outputs\")\nfor k, val in outputs.text_model_output.items():\n    print(f\"{k}: shape of {val.shape}\")\n\nprint(\"\\nVision model outputs\")\nfor k, val in outputs.vision_model_output.items():\n    print(f\"{k}: shape of {val.shape}\") ","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t7oKBoKQEz_w","outputId":"5deb56c6-c302-4175-a854-d4d6e7b7d903"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Draw predictions on image\n\nLet's draw the predictions / found objects on the input image. Remember the found objects correspond to the input text queries. ","metadata":{"id":"h2C5dkQ8sBVZ"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nfrom transformers.image_utils import ImageFeatureExtractionMixin\nmixin = ImageFeatureExtractionMixin()\n\n# Load example image\nimage_size = model.config.vision_config.image_size\nimage = mixin.resize(image, image_size)\ninput_image = np.asarray(image).astype(np.float32) / 255.0\n\n# Threshold to eliminate low probability predictions\nscore_threshold = 0.1\n\n# Get prediction logits\nlogits = torch.max(outputs[\"logits\"][0], dim=-1)\nscores = torch.sigmoid(logits.values).cpu().detach().numpy()\n\n# Get prediction labels and boundary boxes\nlabels = logits.indices.cpu().detach().numpy()\nboxes = outputs[\"pred_boxes\"][0].cpu().detach().numpy()","metadata":{"id":"PNYQe2xbl5vl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_predictions(input_image, text_queries, scores, boxes, labels):\n    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n    ax.imshow(input_image, extent=(0, 1, 1, 0))\n    ax.set_axis_off()\n\n    for score, box, label in zip(scores, boxes, labels):\n      if score < score_threshold:\n        continue\n\n      cx, cy, w, h = box\n      ax.plot([cx-w/2, cx+w/2, cx+w/2, cx-w/2, cx-w/2],\n              [cy-h/2, cy-h/2, cy+h/2, cy+h/2, cy-h/2], \"r\")\n      ax.text(\n          cx - w / 2,\n          cy + h / 2 + 0.015,\n          f\"{text_queries[label]}: {score:1.2f}\",\n          ha=\"left\",\n          va=\"top\",\n          color=\"red\",\n          bbox={\n              \"facecolor\": \"white\",\n              \"edgecolor\": \"red\",\n              \"boxstyle\": \"square,pad=.3\"\n          })\n    \nplot_predictions(input_image, text_queries, scores, boxes, labels)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":480},"id":"Xc6YjWXBl_vK","outputId":"23ed2b75-59f0-461b-ce8d-48574de96f92"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Batch processing\nWe can also pass in multiple sets of images and text queries to search for different (or same) objects in different images. Let's download an image of a coffee mug to process alongside the astronaut image.\n\nFor batch processing, we need to input text queries as a nested list to `OwlViTProcessor` and images as lists of (PIL images or PyTorch tensors or NumPy arrays).","metadata":{"id":"WBgWR3yONIwT"}},{"cell_type":"code","source":"# Download the coffee mug image\nimage = skimage.data.coffee()\nimage = Image.fromarray(np.uint8(image)).convert(\"RGB\")\nimage","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":417},"id":"fZJrboQlswuA","outputId":"ac0c0d96-f84f-4938-dadc-9a80669cb5a7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preprocessing\nimages = [skimage.data.astronaut(), skimage.data.coffee()]\nimages = [Image.fromarray(np.uint8(img)).convert(\"RGB\") for img in images]\n\n# Nexted list of text queries to search each image for\ntext_queries = [[\"human face\", \"rocket\", \"nasa badge\", \"star-spangled banner\"], [\"coffee mug\", \"spoon\", \"plate\"]]\n\n# Process image and text inputs\ninputs = processor(text=text_queries, images=images, return_tensors=\"pt\").to(device)\n\n# Print input names and shapes\nfor key, val in inputs.items():\n    print(f\"{key}: {val.shape}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NVLvC8IrNPQW","outputId":"a0c49462-2137-4a02-8971-f40765ef35fa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Note:** Notice the size of the `input_ids `and `attention_mask` is `[batch_size * num_max_text_queries, max_length]`. Max_length is set to 16 for all OWL-ViT models.","metadata":{"id":"i9Q2ik1kNZ1j"}},{"cell_type":"code","source":"# Get predictions\nwith torch.no_grad():\n  outputs = model(**inputs)\n\nfor k, val in outputs.items():\n    if k not in {\"text_model_output\", \"vision_model_output\"}:\n        print(f\"{k}: shape of {val.shape}\")\n        \nprint(\"\\nText model outputs\")\nfor k, val in outputs.text_model_output.items():\n    print(f\"{k}: shape of {val.shape}\")\n\nprint(\"\\nVision model outputs\")\nfor k, val in outputs.vision_model_output.items():\n    print(f\"{k}: shape of {val.shape}\") ","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ApJXe-2sNR9-","outputId":"61756193-4fae-4d9d-90a2-13da26ae185e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's plot the predictions for the second image\nimage_idx = 1\nimage_size = model.config.vision_config.image_size\nimage = mixin.resize(images[image_idx], image_size)\ninput_image = np.asarray(image).astype(np.float32) / 255.0\n\n# Threshold to eliminate low probability predictions\nscore_threshold = 0.1\n\n# Get prediction logits\nlogits = torch.max(outputs[\"logits\"][image_idx], dim=-1)\nscores = torch.sigmoid(logits.values).cpu().detach().numpy()\n\n# Get prediction labels and boundary boxes\nlabels = logits.indices.cpu().detach().numpy()\nboxes = outputs[\"pred_boxes\"][image_idx].cpu().detach().numpy()\n\nplot_predictions(input_image, text_queries[image_idx], scores, boxes, labels)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Post-processing model predictions\nNotice how we printed the output predictions on the resized input image. This is because OWL-ViT outputs normalized box coordinates in `[cx, cy, w, h]` format assuming a fixed input image size. We can use the `OwlViTProcessor`'s convenient post_process() method to convert the model outputs to a COCO API format and retrieve rescaled coordinates (with respect to the original image sizes) in `[x0, y0, x1, y1]` format. ","metadata":{"id":"n6VYaXGbNfwU"}},{"cell_type":"code","source":"# Target image sizes (height, width) to rescale box predictions [batch_size, 2]\ntarget_sizes = torch.Tensor([img.size[::-1] for img in images]).to(device)\n\n# Convert outputs (bounding boxes and class logits) to COCO API\nresults = processor.post_process(outputs=outputs, target_sizes=target_sizes)\n\n# Loop over predictions for each image in the batch\nfor i in range(len(images)):\n    print(f\"\\nProcessing image {i}\")\n    text = text_queries[i]\n    boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n\n    score_threshold = 0.1\n    for box, score, label in zip(boxes, scores, labels):\n        box = [round(i, 2) for i in box.tolist()]\n\n        if score >= score_threshold:\n            print(f\"Detected {text[label]} with confidence {round(score.item(), 3)} at location {box}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R3xneq28NcUm","outputId":"802a1707-e309-4c96-88bd-a8882afee8d9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bonus: one-shot / image-guided object detection\nInstead of performing zero-shot detection with text inputs, we can use the `OwlViTForObjectDetection.image_guided_detection()` method to query an input image with a query / example image and detect similar looking objects. To do this, we simply pass in `query_images` instead of text to the processor to get the `query_pixel_values`. Note that, unlike text input, `OwlViTProcessor` expects one query image per target image we'd like to query for similar objects. We will also see that the output and post-processing of one-shot object detection is very similar to the zero-shot / text-guided detection.\n\nLet's try this out by querying an image with cats with another random cat image. For this part of the demo, we will perform image-guided object detection, post-process the results and display the predicted boundary boxes on the original input image using OpenCV.","metadata":{}},{"cell_type":"code","source":"import cv2\nimport requests\nfrom matplotlib import rcParams\n\n# Set figure size\n%matplotlib inline\nrcParams['figure.figsize'] = 11 ,8\n\n# Input image\nurl = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\ntarget_sizes = torch.Tensor([image.size[::-1]])\n\n# Query image\nquery_url = \"http://images.cocodataset.org/val2017/000000524280.jpg\"\nquery_image = Image.open(requests.get(query_url, stream=True).raw)\n\n# Display input image and query image\nfig, ax = plt.subplots(1,2)\nax[0].imshow(image)\nax[1].imshow(query_image)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Process input and query image\ninputs = processor(images=image, query_images=query_image, return_tensors=\"pt\").to(device)\n\n# Print input names and shapes\nfor key, val in inputs.items():\n    print(f\"{key}: {val.shape}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get predictions\nwith torch.no_grad():\n  outputs = model.image_guided_detection(**inputs)\n\nfor k, val in outputs.items():\n    if k not in {\"text_model_output\", \"vision_model_output\"}:\n        print(f\"{k}: shape of {val.shape}\")\n\nprint(\"\\nVision model outputs\")\nfor k, val in outputs.vision_model_output.items():\n    print(f\"{k}: shape of {val.shape}\") ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"img = cv2.cvtColor(np.array(image), cv2.COLOR_BGR2RGB)\noutputs.logits = outputs.logits.cpu()\noutputs.target_pred_boxes = outputs.target_pred_boxes.cpu() \n\nresults = processor.post_process_image_guided_detection(outputs=outputs, threshold=0.6, nms_threshold=0.3, target_sizes=target_sizes)\nboxes, scores = results[0][\"boxes\"], results[0][\"scores\"]\n\n# Draw predicted bounding boxes\nfor box, score in zip(boxes, scores):\n    box = [int(i) for i in box.tolist()]\n\n    img = cv2.rectangle(img, box[:2], box[2:], (255,0,0), 5)\n    if box[3] + 25 > 768:\n        y = box[3] - 10\n    else:\n        y = box[3] + 25 \n        \nplt.imshow(img[:,:,::-1])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}