{"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"include_colab_link":true,"machine_shape":"hm","name":"01_how-to-train.ipynb","provenance":[],"toc_visible":true},"widgets":{"application/vnd.jupyter.widget-state+json":{"016d7c8318f742c1943464b08232a510":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"04e7e6d291da49d5816dc98a2904e95c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_39c23c6a972b419eb2eeeebafeaedc22","placeholder":"​","style":"IPY_MODEL_8388e9da9da4492c98c19235ca5fc1b5","value":" 15228/15228 [2:46:46&lt;00:00,  1.52it/s]"}},"0989d41a4da24e9ebff377e02127642c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d295dd80550447d88da0f04ce36a22ff","IPY_MODEL_04e7e6d291da49d5816dc98a2904e95c"],"layout":"IPY_MODEL_42c6061ef7e44f179db5a6e3551c0f17"}},"39c23c6a972b419eb2eeeebafeaedc22":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40bf955ba0284e84b198da6be8654219":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"42c6061ef7e44f179db5a6e3551c0f17":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6feb10aeb43147e6aba028d065947ae8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"837c9ddc3d594e088891874560c646b8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Epoch: 100%","description_tooltip":null,"layout":"IPY_MODEL_fe20a8dae6e84628b5076d02183090f5","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_40bf955ba0284e84b198da6be8654219","value":1}},"8388e9da9da4492c98c19235ca5fc1b5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"93b3f9eae3cb4e3e859cf456e3547c6d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a491e8caa0a048beb3b5259f14eb233f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a58a66392b644b1384661e850c077a6c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_837c9ddc3d594e088891874560c646b8","IPY_MODEL_dbf50873d62c4ba39321faefbed0cca5"],"layout":"IPY_MODEL_a491e8caa0a048beb3b5259f14eb233f"}},"d295dd80550447d88da0f04ce36a22ff":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Iteration: 100%","description_tooltip":null,"layout":"IPY_MODEL_016d7c8318f742c1943464b08232a510","max":15228,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e7d8c3a4fecd40778e32966b29ea65a1","value":15228}},"dbf50873d62c4ba39321faefbed0cca5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6feb10aeb43147e6aba028d065947ae8","placeholder":"​","style":"IPY_MODEL_93b3f9eae3cb4e3e859cf456e3547c6d","value":" 1/1 [2:46:46&lt;00:00, 10006.17s/it]"}},"e7d8c3a4fecd40778e32966b29ea65a1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"fe20a8dae6e84628b5076d02183090f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n> 最后一次修改: [dingzh@dp.tech](mailto:dingzh@dp.tech)\n>\n> 描述: 本教程主要参考 [hugging face notebook](https://github.com/huggingface/notebooks/blob/main/transformers_doc/en/quicktour.ipynb)，可在 Bohrium Notebook 上直接运行。你可以点击界面上方蓝色按钮 `开始连接`，选择 `bohrium-notebook:2023-04-07` 镜像及任意一款`GPU`节点配置，稍等片刻即可运行。\n> 如您遇到任何问题，请联系 [bohrium@dp.tech](mailto:bohrium@dp.tech) 。\n>\n> 共享协议: 本作品采用[知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议](https://creativecommons.org/licenses/by-nc-sa/4.0/)进行许可。","metadata":{"colab_type":"text","id":"view-in-github"}},{"cell_type":"code","source":"# #@title\n# %%html\n# <div style=\"background-color: pink;\">\n#   Notebook written in collaboration with <a href=\"https://github.com/aditya-malte\">Aditya Malte</a>.\n#   <br>\n#   The Notebook is on GitHub, so contributions are more than welcome.\n# </div>\n# <br>\n# <div style=\"background-color: yellow;\">\n#   Aditya wrote another notebook with a slightly different use case and methodology, please check it out.\n#   <br>\n#   <a target=\"_blank\" href=\"https://gist.github.com/aditya-malte/2d4f896f471be9c38eb4d723a710768b\">\n#     https://gist.github.com/aditya-malte/2d4f896f471be9c38eb4d723a710768b\n#   </a>\n# </div>\n","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":100},"colab_type":"code","id":"e67Ut53QYEdU","outputId":"437871b8-b8ac-4eaf-c2e1-61d801c5e6b2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# How to train a new language model from scratch using Transformers and Tokenizers\n\n### Notebook edition (link to blogpost [link](https://huggingface.co/blog/how-to-train)). Last update May 15, 2020\n\n\nOver the past few months, we made several improvements to our [`transformers`](https://github.com/huggingface/transformers) and [`tokenizers`](https://github.com/huggingface/tokenizers) libraries, with the goal of making it easier than ever to **train a new language model from scratch**.\n\nIn this post we’ll demo how to train a “small” model (84 M parameters = 6 layers, 768 hidden size, 12 attention heads) – that’s the same number of layers & heads as DistilBERT – on **Esperanto**. We’ll then fine-tune the model on a downstream task of part-of-speech tagging.\n","metadata":{"colab_type":"text","id":"M1oqh0F6W3ad"}},{"cell_type":"markdown","source":"## 1. Find a dataset\n\nFirst, let us find a corpus of text in Esperanto. Here we’ll use the Esperanto portion of the [OSCAR corpus](https://traces1.inria.fr/oscar/) from INRIA.\nOSCAR is a huge multilingual corpus obtained by language classification and filtering of [Common Crawl](https://commoncrawl.org/) dumps of the Web.\n\n<img src=\"https://huggingface.co/blog/assets/01_how-to-train/oscar.png\" style=\"margin: auto; display: block; width: 260px;\">\n\nThe Esperanto portion of the dataset is only 299M, so we’ll concatenate with the Esperanto sub-corpus of the [Leipzig Corpora Collection](https://wortschatz.uni-leipzig.de/en/download), which is comprised of text from diverse sources like news, literature, and wikipedia.\n\nThe final training corpus has a size of 3 GB, which is still small – for your model, you will get better results the more data you can get to pretrain on. \n\n","metadata":{"colab_type":"text","id":"oK7PPVm2XBgr"}},{"cell_type":"code","source":"# in this notebook we'll only get one of the files (the Oscar one) for the sake of simplicity and performance\n!wget -c https://cdn-datasets.huggingface.co/EsperBERTo/data/oscar.eo.txt","metadata":{"colab":{},"colab_type":"code","id":"HOk4iZ9YZvec","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Train a tokenizer\n\nWe choose to train a byte-level Byte-pair encoding tokenizer (the same as GPT-2), with the same special tokens as RoBERTa. Let’s arbitrarily pick its size to be 52,000.\n\nWe recommend training a byte-level BPE (rather than let’s say, a WordPiece tokenizer like BERT) because it will start building its vocabulary from an alphabet of single bytes, so all words will be decomposable into tokens (no more `<unk>` tokens!).\n","metadata":{"colab_type":"text","id":"G-kkz81OY6xH"}},{"cell_type":"code","source":"# # We won't need TensorFlow here\n# !pip uninstall -y tensorflow\n# # Install `transformers` from master\n# !pip install git+https://github.com/huggingface/transformers\n# !pip list | grep -E 'transformers|tokenizers'\n# # transformers version at notebook update --- 2.11.0\n# # tokenizers version at notebook update --- 0.8.0rc1","metadata":{"colab":{},"colab_type":"code","id":"5duRggBRZKvP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time \nfrom pathlib import Path\n\nfrom tokenizers import ByteLevelBPETokenizer\n\npaths = [str(x) for x in Path(\".\").glob(\"**/*.txt\")]\n\n# Initialize a tokenizer\ntokenizer = ByteLevelBPETokenizer()\n\n# Customize training\ntokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n    \"<s>\",\n    \"<pad>\",\n    \"</s>\",\n    \"<unk>\",\n    \"<mask>\",\n])","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"colab_type":"code","id":"IMnymRDLe0hi","outputId":"4d26476f-e6b5-475a-a0c1-41b6fcdc041a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pwd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's save files to disk","metadata":{"colab_type":"text","id":"6Ei7bqpRf1LH"}},{"cell_type":"code","source":"!mkdir EsperBERTo\ntokenizer.save_model(\"EsperBERTo\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","id":"EIS-irI0f32P","outputId":"e86c4a24-eb65-4f0a-aa58-ed1931a05ac9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"🔥🔥 Wow, that was fast! ⚡️🔥\n\nWe now have both a `vocab.json`, which is a list of the most frequent tokens ranked by frequency, and a `merges.txt` list of merges.\n\n```json\n{\n\t\"<s>\": 0,\n\t\"<pad>\": 1,\n\t\"</s>\": 2,\n\t\"<unk>\": 3,\n\t\"<mask>\": 4,\n\t\"!\": 5,\n\t\"\\\"\": 6,\n\t\"#\": 7,\n\t\"$\": 8,\n\t\"%\": 9,\n\t\"&\": 10,\n\t\"'\": 11,\n\t\"(\": 12,\n\t\")\": 13,\n\t# ...\n}\n\n# merges.txt\nl a\nĠ k\no n\nĠ la\nt a\nĠ e\nĠ d\nĠ p\n# ...\n```\n\nWhat is great is that our tokenizer is optimized for Esperanto. Compared to a generic tokenizer trained for English, more native words are represented by a single, unsplit token. Diacritics, i.e. accented characters used in Esperanto – `ĉ`, `ĝ`, `ĥ`, `ĵ`, `ŝ`, and `ŭ` – are encoded natively. We also represent sequences in a more efficient manner. Here on this corpus, the average length of encoded sequences is ~30% smaller as when using the pretrained GPT-2 tokenizer.\n\nHere’s  how you can use it in `tokenizers`, including handling the RoBERTa special tokens – of course, you’ll also be able to use it directly from `transformers`.\n","metadata":{"colab_type":"text","id":"lOOfYSuQhSqT"}},{"cell_type":"code","source":"from tokenizers.implementations import ByteLevelBPETokenizer\nfrom tokenizers.processors import BertProcessing\n\n\ntokenizer = ByteLevelBPETokenizer(\n    \"./EsperBERTo/vocab.json\",\n    \"./EsperBERTo/merges.txt\",\n)","metadata":{"colab":{},"colab_type":"code","id":"tKVWB8WShT-z","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer._tokenizer.post_processor = BertProcessing(\n    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n)\ntokenizer.enable_truncation(max_length=512)","metadata":{"colab":{},"colab_type":"code","id":"hO5M3vrAhcuj","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.encode(\"Mi estas Julien.\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","id":"E3Ye27nchfzq","outputId":"b9812ed2-1ecd-4e1b-d9bd-7de581955e70","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.encode(\"Mi estas Julien.\").tokens","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","id":"X8ya5_7rhjKS","outputId":"e9e08ded-1081-4823-dd81-9d6be1255385","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Train a language model from scratch\n\n**Update:** This section follows along the [`run_language_modeling.py`](https://github.com/huggingface/transformers/blob/master/examples/legacy/run_language_modeling.py) script, using our new [`Trainer`](https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py) directly. Feel free to pick the approach you like best.\n\n> We’ll train a RoBERTa-like model, which is a BERT-like with a couple of changes (check the [documentation](https://huggingface.co/transformers/model_doc/roberta.html) for more details).\n\nAs the model is BERT-like, we’ll train it on a task of *Masked language modeling*, i.e. the predict how to fill arbitrary tokens that we randomly mask in the dataset. This is taken care of by the example script.\n","metadata":{"colab_type":"text","id":"WQpUC_CDhnWW"}},{"cell_type":"code","source":"# Check that we have a GPU\n!nvidia-smi","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":318},"colab_type":"code","id":"kD140sFjh0LQ","outputId":"0bab1f9e-bf7a-4f13-82d3-07fe5866ce78","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check that PyTorch sees it\nimport torch\ntorch.cuda.is_available()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","id":"VNZZs-r6iKAV","outputId":"c8404d6c-7662-4240-c8da-ee89edfaf51b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We'll define the following config for the model","metadata":{"colab_type":"text","id":"u0qQzgrBi1OX"}},{"cell_type":"code","source":"from transformers import RobertaConfig\n\nconfig = RobertaConfig(\n    vocab_size=52_000,\n    max_position_embeddings=514,\n    num_attention_heads=12,\n    num_hidden_layers=6,\n    type_vocab_size=1,\n)","metadata":{"colab":{},"colab_type":"code","id":"LTXXutqeDzPi","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's re-create our tokenizer in transformers","metadata":{"colab_type":"text","id":"yAwQ82JiE5pi"}},{"cell_type":"code","source":"from transformers import RobertaTokenizerFast\n\ntokenizer = RobertaTokenizerFast.from_pretrained(\"./EsperBERTo\", max_len=512)","metadata":{"colab":{},"colab_type":"code","id":"4keFBUjQFOD1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finally let's initialize our model.\n\n**Important:**\n\nAs we are training from scratch, we only initialize from a config, not from an existing pretrained model or checkpoint.","metadata":{"colab_type":"text","id":"6yNCw-3hFv9h"}},{"cell_type":"code","source":"from transformers import RobertaForMaskedLM\n\nmodel = RobertaForMaskedLM(config=config)","metadata":{"colab":{},"colab_type":"code","id":"BzMqR-dzF4Ro","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.num_parameters()\n# => 84 million parameters","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","id":"jU6JhBSTKiaM","outputId":"35879a60-2915-4894-f702-2d649cfa398a","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Now let's build our training Dataset\n\nWe'll build our dataset by applying our tokenizer to our text file.\n\nHere, as we only have one text file, we don't even need to customize our `Dataset`. We'll just use the `LineByLineDataset` out-of-the-box.","metadata":{"colab_type":"text","id":"jBtUHRMliOLM"}},{"cell_type":"code","source":"%%time\nfrom transformers import LineByLineTextDataset\n\ndataset = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path=\"./oscar.eo.txt\",\n    block_size=128,\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"colab_type":"code","id":"GlvP_A-THEEl","outputId":"e0510a33-7937-4a04-fa1c-d4e20b758bb2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Like in the [`run_language_modeling.py`](https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_language_modeling.py) script, we need to define a data_collator.\n\nThis is just a small helper that will help us batch different samples of the dataset together into an object that PyTorch knows how to perform backprop on.","metadata":{"colab_type":"text","id":"hDLs73HcIHk5"}},{"cell_type":"code","source":"from transformers import DataCollatorForLanguageModeling\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n)","metadata":{"colab":{},"colab_type":"code","id":"zTgWPa9Dipk2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Finally, we are all set to initialize our Trainer","metadata":{"colab_type":"text","id":"ri2BIQKqjfHm"}},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./EsperBERTo\",\n    overwrite_output_dir=True,\n    num_train_epochs=1,\n    per_gpu_train_batch_size=64,\n    save_steps=10_000,\n    save_total_limit=2,\n    prediction_loss_only=True,\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=dataset,\n)","metadata":{"colab":{},"colab_type":"code","id":"YpvnFFmZJD-N","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Start training","metadata":{"colab_type":"text","id":"o6sASa36Nf-N"}},{"cell_type":"code","source":"%%time\ntrainer.train()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":738,"referenced_widgets":["a58a66392b644b1384661e850c077a6c","a491e8caa0a048beb3b5259f14eb233f","837c9ddc3d594e088891874560c646b8","dbf50873d62c4ba39321faefbed0cca5","40bf955ba0284e84b198da6be8654219","fe20a8dae6e84628b5076d02183090f5","93b3f9eae3cb4e3e859cf456e3547c6d","6feb10aeb43147e6aba028d065947ae8","0989d41a4da24e9ebff377e02127642c","42c6061ef7e44f179db5a6e3551c0f17","d295dd80550447d88da0f04ce36a22ff","04e7e6d291da49d5816dc98a2904e95c","e7d8c3a4fecd40778e32966b29ea65a1","016d7c8318f742c1943464b08232a510","8388e9da9da4492c98c19235ca5fc1b5","39c23c6a972b419eb2eeeebafeaedc22"]},"colab_type":"code","id":"VmaHZXzmkNtJ","outputId":"a19880cb-bcc6-4885-bf24-c2c6d0f56d1e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### 🎉 Save final model (+ tokenizer + config) to disk","metadata":{"colab_type":"text","id":"_ZkooHz1-_2h"}},{"cell_type":"code","source":"trainer.save_model(\"./EsperBERTo\")","metadata":{"colab":{},"colab_type":"code","id":"QDNgPls7_l13","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Check that the LM actually trained","metadata":{"colab_type":"text","id":"d0caceCy_p1-"}},{"cell_type":"markdown","source":"Aside from looking at the training and eval losses going down, the easiest way to check whether our language model is learning anything interesting is via the `FillMaskPipeline`.\n\nPipelines are simple wrappers around tokenizers and models, and the 'fill-mask' one will let you input a sequence containing a masked token (here, `<mask>`) and return a list of the most probable filled sequences, with their probabilities.\n\n","metadata":{"colab_type":"text","id":"iIQJ8ND_AEhl"}},{"cell_type":"code","source":"from transformers import pipeline\n\nfill_mask = pipeline(\n    \"fill-mask\",\n    model=\"./EsperBERTo\",\n    tokenizer=\"./EsperBERTo\"\n)","metadata":{"colab":{},"colab_type":"code","id":"ltXgXyCbAJLY","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The sun <mask>.\n# =>\n\nfill_mask(\"La suno <mask>.\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":283},"colab_type":"code","id":"UIvgZ3S6AO0z","outputId":"5f3d2f00-abdc-44a9-9c1b-75e3ec328576","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ok, simple syntax/grammar works. Let’s try a slightly more interesting prompt:\n\n","metadata":{"colab_type":"text","id":"i0qCyyhNAWZi"}},{"cell_type":"code","source":"fill_mask(\"Jen la komenco de bela <mask>.\")\n\n# This is the beginning of a beautiful <mask>.\n# =>","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":283},"colab_type":"code","id":"YZ9HSQxAAbme","outputId":"aabfeedc-b1d0-4837-b01d-cd42726a5a3d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}