{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.16"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Text-guided image-to-image generation","metadata":{}},{"cell_type":"markdown","source":"> 最后一次修改: [dingzh@dp.tech](mailto:dingzh@dp.tech)\n>\n> 描述: 本教程主要参考 [hugging face notebook](https://github.com/huggingface/notebooks/blob/main/diffusers_doc/en/img2img.ipynb)，可在 Bohrium Notebook 上直接运行。你可以点击界面上方蓝色按钮 `开始连接`，选择 `bohrium-notebook:2023-04-07` 镜像及任意一款`GPU`节点配置，稍等片刻即可运行。\n> 如您遇到任何问题，请联系 [bohrium@dp.tech](mailto:bohrium@dp.tech) 。\n>\n> 共享协议: 本作品采用[知识共享署名-非商业性使用-相同方式共享 4.0 国际许可协议](https://creativecommons.org/licenses/by-nc-sa/4.0/)进行许可。\n\nThis notebook shows how to fine-tune any pretrained Vision model for Image Classification on a custom dataset. The idea is to add a randomly initialized classification head on top of a pre-trained encoder, and fine-tune the model altogether on a labeled dataset.","metadata":{}},{"cell_type":"markdown","source":"The [StableDiffusionImg2ImgPipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/img2img#diffusers.StableDiffusionImg2ImgPipeline) lets you pass a text prompt and an initial image to condition the generation of new images.\n\nBefore you begin, make sure you have all the necessary libraries installed:\n\n```bash\n!pip install diffusers transformers ftfy accelerate\n```\n\nGet started by creating a [StableDiffusionImg2ImgPipeline](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/img2img#diffusers.StableDiffusionImg2ImgPipeline) with a pretrained Stable Diffusion model like [`nitrosocke/Ghibli-Diffusion`](https://huggingface.co/nitrosocke/Ghibli-Diffusion).","metadata":{}},{"cell_type":"code","source":"pip install diffusers transformers","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os \nos.environ['HTTP_PROXY'] = 'http://ga.dp.tech:8118'\nos.environ['HTTPS_PROXY'] = 'http://ga.dp.tech:8118'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport requests\nfrom PIL import Image\nfrom io import BytesIO\nfrom diffusers import StableDiffusionImg2ImgPipeline\n\ndevice = \"cuda\"\npipe = StableDiffusionImg2ImgPipeline.from_pretrained(\"nitrosocke/Ghibli-Diffusion\", torch_dtype=torch.float16).to(\n    device\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Download and preprocess an initial image so you can pass it to the pipeline:","metadata":{}},{"cell_type":"code","source":"url = \"https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg\"\n\nresponse = requests.get(url)\ninit_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\ninit_image.thumbnail((768, 768))\ninit_image","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/YiYiXu/test-doc-assets/resolve/main/image_2_image_using_diffusers_cell_8_output_0.jpeg\"/>\n</div>\n\n<Tip>\n\n💡 `strength` is a value between 0.0 and 1.0 that controls the amount of noise added to the input image. Values that approach 1.0 allow for lots of variations but will also produce images that are not semantically consistent with the input.\n\n</Tip>\n\nDefine the prompt (for this checkpoint finetuned on Ghibli-style art, you need to prefix the prompt with the `ghibli style` tokens) and run the pipeline:","metadata":{}},{"cell_type":"code","source":"prompt = \"ghibli style, a fantasy landscape with castles\"\ngenerator = torch.Generator(device=device).manual_seed(1024)\nimage = pipe(prompt=prompt, image=init_image, strength=0.75, guidance_scale=7.5, generator=generator).images[0]\nimage","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/ghibli-castles.png\"/>\n</div>\n\nYou can also try experimenting with a different scheduler to see how that affects the output:","metadata":{}},{"cell_type":"code","source":"from diffusers import LMSDiscreteScheduler\n\nlms = LMSDiscreteScheduler.from_config(pipe.scheduler.config)\npipe.scheduler = lms\ngenerator = torch.Generator(device=device).manual_seed(1024)\nimage = pipe(prompt=prompt, image=init_image, strength=0.75, guidance_scale=7.5, generator=generator).images[0]\nimage","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"flex justify-center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/lms-ghibli.png\"/>\n</div>\n\nCheck out the Spaces below, and try generating images with different values for `strength`. You'll notice that using lower values for `strength` produces images that are more similar to the original image.\n\nFeel free to also switch the scheduler to the [LMSDiscreteScheduler](https://huggingface.co/docs/diffusers/main/en/api/schedulers/lms_discrete#diffusers.LMSDiscreteScheduler) and see how that affects the output.\n\n<iframe\n\tsrc=\"https://stevhliu-ghibli-img2img.hf.space\"\n\tframeborder=\"0\"\n\twidth=\"850\"\n\theight=\"500\"\n></iframe>","metadata":{}}]}